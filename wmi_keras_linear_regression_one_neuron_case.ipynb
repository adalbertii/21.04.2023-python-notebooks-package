{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalbertii/21.04.2023-python-notebooks-package/blob/main/wmi_keras_linear_regression_one_neuron_case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Educational Friday: 21.04.2023\n",
        "\n",
        "\n",
        "---\n",
        "Developed by Wojciech Michalski\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Construction of neural networks - regresion model case\n",
        "\n",
        "Using the Keras library\n",
        "\n"
      ],
      "metadata": {
        "id": "SRikR7c45C2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Loading the Required Libraries and Modules"
      ],
      "metadata": {
        "id": "DoYccyJtoIRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AIzoNewV2pkC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - Preparing the Data "
      ],
      "metadata": {
        "id": "HqHzoXOPoL7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
        "my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])\n",
        "\n",
        " \n",
        "plt.xlabel(\"my_feature\")\n",
        "plt.ylabel(\"my_label\")\n",
        "\n",
        "plt.scatter(my_feature, my_label)\n"
      ],
      "metadata": {
        "id": "_abOqU213YC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - model building "
      ],
      "metadata": {
        "id": "KbncwBODo29g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01\n",
        "epochs=10\n",
        "my_batch_size=12"
      ],
      "metadata": {
        "id": "H3-jk1Me4oaj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "# Compile the model topography into code that \n",
        "# TensorFlow can efficiently execute. Configure \n",
        "# training to minimize the model's mean squared error. \n",
        "model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "  "
      ],
      "metadata": {
        "id": "o1Oi19yS42Bo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5iQ3q3aakjXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - model training"
      ],
      "metadata": {
        "id": "2ySr9mnWpKqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed the feature values and the label values to the \n",
        "# model. The model will train for the specified number \n",
        "# of epochs, gradually learning how the feature values\n",
        "# relate to the label values. \n",
        "history = model.fit(x=my_feature,\n",
        "                      y=my_label,\n",
        "                      batch_size=my_batch_size,\n",
        "                      epochs=epochs)"
      ],
      "metadata": {
        "id": "_9XAFQ4x5-pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather the trained model's weight and bias.\n",
        "trained_weight = model.get_weights()[0]\n",
        "trained_bias = model.get_weights()[1]"
      ],
      "metadata": {
        "id": "VVHHWVWj-ubC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The list of epochs is stored separately from the \n",
        "# rest of history.\n",
        "epochs = history.epoch"
      ],
      "metadata": {
        "id": "fH_WyXjP_pgk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather the history (a snapshot) of each epoch.\n",
        "hist = pd.DataFrame(history.history)"
      ],
      "metadata": {
        "id": "B7ILAlcZ_AGD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifically gather the model's root mean \n",
        "# squared error at each epoch. \n",
        "rmse = hist[\"root_mean_squared_error\"]"
      ],
      "metadata": {
        "id": "KI_iEAjI_Lhe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label the axes.\n",
        "plt.xlabel(\"feature\")\n",
        "plt.ylabel(\"label\")\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "plt.scatter(my_feature, my_label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "x0 = 0\n",
        "y0 = trained_bias\n",
        "x1 = my_feature[-1]\n",
        "y1 = trained_bias + (trained_weight * x1)\n",
        "plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "plt.show()\n",
        "\n",
        "\"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "plt.plot(epochs, rmse, label=\"Loss\")\n",
        "plt.legend()\n",
        "plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ak-IiloH7p07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 - optimization of hyperparameters of the learning process"
      ],
      "metadata": {
        "id": "Stfl3tU2p675"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------> **we increase the number of epochs**"
      ],
      "metadata": {
        "id": "dsH5HyVmQO_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01\n",
        "epochs= 50   # Replace ? with an integer.\n",
        "my_batch_size=12\n"
      ],
      "metadata": {
        "id": "fb6CM1ei9wCg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "# Compile the model topography into code that \n",
        "# TensorFlow can efficiently execute. Configure \n",
        "# training to minimize the model's mean squared error. \n",
        "model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.summary()  "
      ],
      "metadata": {
        "id": "8lYgXya_PKOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=my_feature,\n",
        "                      y=my_label,\n",
        "                      batch_size=my_batch_size,\n",
        "                      epochs=epochs)\n",
        "\n"
      ],
      "metadata": {
        "id": "WBcZ6T_VPSKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather the trained model's weight and bias.\n",
        "trained_weight = model.get_weights()[0]\n",
        "trained_bias = model.get_weights()[1]\n",
        "epochs = history.epoch\n",
        "hist = pd.DataFrame(history.history)\n",
        "rmse = hist[\"root_mean_squared_error\"]\n",
        "# Label the axes.\n",
        "plt.xlabel(\"feature\")\n",
        "plt.ylabel(\"label\")\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "plt.scatter(my_feature, my_label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "x0 = 0\n",
        "y0 = trained_bias\n",
        "x1 = my_feature[-1]\n",
        "y1 = trained_bias + (trained_weight * x1)\n",
        "plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "plt.show()\n",
        "\n",
        "\"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "plt.plot(epochs, rmse, label=\"Loss\")\n",
        "plt.legend()\n",
        "plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_5zlNOnLP95G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------> **we continue to increase the number of epochs**"
      ],
      "metadata": {
        "id": "Ya7gAdSmQrd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01\n",
        "epochs=450\n",
        "my_batch_size=12"
      ],
      "metadata": {
        "id": "WrMS87p395kK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "# Compile the model topography into code that \n",
        "# TensorFlow can efficiently execute. Configure \n",
        "# training to minimize the model's mean squared error. \n",
        "model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.summary()  \n"
      ],
      "metadata": {
        "id": "7M610g_MQ7Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=my_feature,\n",
        "                      y=my_label,\n",
        "                      batch_size=my_batch_size,\n",
        "                      epochs=epochs)\n"
      ],
      "metadata": {
        "id": "K3bga_PhRDKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather the trained model's weight and bias.\n",
        "trained_weight = model.get_weights()[0]\n",
        "trained_bias = model.get_weights()[1]\n",
        "epochs = history.epoch\n",
        "hist = pd.DataFrame(history.history)\n",
        "rmse = hist[\"root_mean_squared_error\"]\n",
        "# Label the axes.\n",
        "plt.xlabel(\"feature\")\n",
        "plt.ylabel(\"label\")\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "plt.scatter(my_feature, my_label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "x0 = 0\n",
        "y0 = trained_bias\n",
        "x1 = my_feature[-1]\n",
        "y1 = trained_bias + (trained_weight * x1)\n",
        "plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "plt.show()\n",
        "\n",
        "\"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "plt.plot(epochs, rmse, label=\"Loss\")\n",
        "plt.legend()\n",
        "plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O02ReyYWRUsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------> **we change the network training step, the number of epochs and the batch parameter**"
      ],
      "metadata": {
        "id": "Pw3iKLy_RiGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.05\n",
        "epochs=25\n",
        "my_batch_size=1 # Wow, a batch size of 1 works!\n"
      ],
      "metadata": {
        "id": "y2HSFzxM-B6U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "# Compile the model topography into code that \n",
        "# TensorFlow can efficiently execute. Configure \n",
        "# training to minimize the model's mean squared error. \n",
        "model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.summary()  \n"
      ],
      "metadata": {
        "id": "oW-HXClYSIZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=my_feature,\n",
        "                      y=my_label,\n",
        "                      batch_size=my_batch_size,\n",
        "                      epochs=epochs)"
      ],
      "metadata": {
        "id": "YqrbcFVKSMHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather the trained model's weight and bias.\n",
        "trained_weight = model.get_weights()[0]\n",
        "trained_bias = model.get_weights()[1]\n",
        "epochs = history.epoch\n",
        "hist = pd.DataFrame(history.history)\n",
        "rmse = hist[\"root_mean_squared_error\"]\n",
        "# Label the axes.\n",
        "plt.xlabel(\"feature\")\n",
        "plt.ylabel(\"label\")\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "plt.scatter(my_feature, my_label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "x0 = 0\n",
        "y0 = trained_bias\n",
        "x1 = my_feature[-1]\n",
        "y1 = trained_bias + (trained_weight * x1)\n",
        "plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "plt.show()\n",
        "\n",
        "\"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "plt.plot(epochs, rmse, label=\"Loss\")\n",
        "plt.legend()\n",
        "plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SVQcdd3ASPcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A few rules:\n",
        "\n",
        "Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n",
        "\n",
        "If the training loss does not converge, train for more epochs.\n",
        "\n",
        "If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n",
        "\n",
        "If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
        "\n",
        "Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
        "\n",
        "Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J9tN7mBX_MfT"
      }
    }
  ]
}